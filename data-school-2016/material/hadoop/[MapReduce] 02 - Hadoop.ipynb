{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop\n",
    "(and Hadoop streaming)\n",
    "\n",
    "<center>\n",
    "<img src='https://media.licdn.com/mpr/mpr/shrinknp_800_800/AAEAAQAAAAAAAAcYAAAAJDgwYjA0ZmViLTJiNzgtNGJmMS1iNjE0LWQ3MzhiZmNjNzNhMg.png'>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**MapReduce** is a completely different paradigm \n",
    "\n",
    "* Solving a certain subset of parallelizable problems \n",
    "    - around the bottleneck of ingesting input data from disk\n",
    "* Traditional parallelism brings the data to the computing machine\n",
    "    - Map/reduce does the opposite, it brings the compute to the data\n",
    "* Input data is not stored on a separate storage system\n",
    "* Data exists in little pieces \n",
    "    - and is permanently stored on each computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MapReduce is the programming paradigm that allows massive scalability across thousands of servers.\n",
    "\n",
    "Its open source server implementation is the *Hadoop* cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also always keep in mind that ***HDFS*** is fundamental to Hadoop \n",
    "\n",
    "* it provides the data chunking distribution across compute elements \n",
    "* necessary for map/reduce applications to be efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our (*short*) road to real Hadoop\n",
    "\n",
    "1. See how Java original Hadoop works\n",
    "2. Copy files from the system to HDFS within the notebooks\n",
    "3. Launch Hadoop\n",
    "4. Understand and launch Hadoop Streaming\n",
    "5. Simulate Hadoop streaming with Bash Pipes\n",
    "6. Launch Hadoop Streaming with Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word count\n",
    "The '`Hello World`' for MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Among the simplest of full Hadoop jobs you can run\n",
    "\n",
    "<img src='http://www.glennklockwood.com/data-intensive/hadoop/wordcount-schematic.png'\n",
    "width='700'>\n",
    "<small>Reading ***Moby Dick*** </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "* The **MAP step** will take the raw text and convert it to key/value pairs\n",
    "    - Each key is a word\n",
    "    - All keys (words) will have a value of 1\n",
    "\n",
    "\n",
    "* The **REDUCE step** will combine all duplicate keys \n",
    "    - By adding up their values (sum)\n",
    "    - Every key (word) has a value of 1 (Map)\n",
    "    - Output is reduced to a list of unique keys\n",
    "    - Each key’s value corresponding to key's (word's) count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src='http://disco.readthedocs.org/en/latest/_images/map_shuffle_reduce.png' width=800>\n",
    "</center>\n",
    "\n",
    "### Map function:\n",
    "processes data and generates a set of  intermediate key/value pairs.\n",
    "\n",
    "\n",
    "### Reduce function:\n",
    "merges all intermediate values  associated with the same intermediate key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A WordCount example \n",
    "*(with Java)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider doing a word count of the following file using  MapReduce:\n",
    "```\n",
    "Hello World Bye World\n",
    "Hello Hadoop Goodbye Hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The map function reads in words one at a time outputs (“word”, 1) for each parsed input word\n",
    "\n",
    "```\n",
    "(Hello, 1)\n",
    "(World, 1)\n",
    "(Bye, 1)\n",
    "(World, 1)\n",
    "(Hello, 1)\n",
    "(Hadoop, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shuffle phase between map and reduce creates a  list of values associated with each key\n",
    "```\n",
    "(Bye, (1))\n",
    "(Goodbye, (1))\n",
    "(Hadoop, (1, 1))\n",
    "(Hello, (1, 1))\n",
    "(World, (1, 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce function sums the numbers in the list for each  key and outputs (word, count) pairs\n",
    "```\n",
    "(Bye, 1)\n",
    "(Goodbye, 1)\n",
    "(Hadoop, 2)\n",
    "(Hello, 2)\n",
    "(World, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can you do this with Java?\n",
    "(the Hadoop framework native language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "// Imports\n",
    "package org.myorg;\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import org.apache.hadoop.*\n",
    "\n",
    "// Create JAVA class\n",
    "public class WordCount {\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Mapper function\n",
    "  public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      String line = value.toString();\n",
    "      StringTokenizer tokenizer = new StringTokenizer(line);\n",
    "      while (tokenizer.hasMoreTokens()) {\n",
    "        word.set(tokenizer.nextToken());\n",
    "        output.collect(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "``` Java\n",
    "//Reducer function\n",
    "  public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "    public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n",
    "      int sum = 0;\n",
    "      while (values.hasNext()) {\n",
    "        sum += values.next().get();\n",
    "      }\n",
    "      output.collect(key, new IntWritable(sum));\n",
    "    }\n",
    "  }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<small>\n",
    "``` Java\n",
    "//Main function\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    JobConf conf = new JobConf(WordCount.class);\n",
    "    conf.setJobName(\"wordcount\");\n",
    "\n",
    "    conf.setOutputKeyClass(Text.class);\n",
    "    conf.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    conf.setMapperClass(Map.class);\n",
    "    conf.setCombinerClass(Reduce.class);\n",
    "    conf.setReducerClass(Reduce.class);\n",
    "\n",
    "    conf.setInputFormat(TextInputFormat.class);\n",
    "    conf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "    FileInputFormat.setInputPaths(conf, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(conf, new Path(args[1]));\n",
    "\n",
    "    JobClient.runJob(conf);\n",
    "  }\n",
    "```\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can test the Java code here. *Live*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%env HADOOP_EXAMPLES /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar\n",
    "%env HADOOP_STREAMING /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%env HADOOP_EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Hadoop available examples\n",
    "! hadoop jar $HADOOP_EXAMPLES | grep word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check wordcount\n",
    "! hadoop jar $HADOOP_EXAMPLES wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brief Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Our input\n",
    "\n",
    "! cat ./data/txt/twolines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "########################\n",
    "# Preprocess with HDFS\n",
    "\n",
    "# Create input directory\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "file=\"./data/txt/twolines.txt\"\n",
    "hdfs dfs -put $file myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Test wordcount with real hadoop on our system\n",
    "! hadoop jar $HADOOP_EXAMPLES wordcount myinput myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Hadoop output\n",
    "! hadoop fs -cat myoutput/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "<img src='https://pbs.twimg.com/media/B2RlCy-IIAEFCLC.jpg' width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop like `pipes` in Unix Bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Prepare a data sample\n",
    "\n",
    "**Warning**: please run this code on your notebook too :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables for python and bash\n",
    "myfile = '/tmp/ngs.sam'\n",
    "%env myfile $myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download compressed NGS data from a link\n",
    "wget -q \"http://bit.ly/ngs_sample_data\" -O $myfile.bz2 && echo \"downloaded\"\n",
    "# Decompress the file\n",
    "bunzip2 $myfile.bz2 && echo \"decompressed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the file is there\n",
    "! ls $myfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Bash piping our own MapReduce with Unix commands\n",
    "head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Understand it better\n",
    "\n",
    "Splitting the command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. `head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "1. `awk '{ print $3\":\"$4 }’`\n",
    "\n",
    "1. `sort`\n",
    "\n",
    "1. `uniq -c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**INPUT STREAM**\n",
    "`head -2000 data/ngs/input.sam | tail -n 10`\n",
    "\n",
    "**MAPPER**\n",
    "`awk '{ print $3\":\"$4 }'`\n",
    "\n",
    "**SHUFFLE**\n",
    "`sort`\n",
    "\n",
    "**REDUCER**\n",
    "`uniq -c`\n",
    "\n",
    "**OUTPUT STREAM**\n",
    "`<STDOUT>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's move step by step and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# STREAMING THE FILE\n",
    "! head -2000 $myfile | tail -n 10\n",
    "# note: \n",
    "# taking the last 10 lines of the first 2000\n",
    "# to skip headers lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MAPPING\n",
    "! head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# SHUFFLING\n",
    "! head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# REDUCER\n",
    "! head -2000 $myfile | tail -n 10 | awk '{ print $3\":\"$4 }' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "<br>\n",
    "\n",
    "<big>\n",
    "Play *a little bit* with bash pipes. \n",
    "</big>\n",
    "\n",
    "Try to understand how MapReduce process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Considerations with bash pipes as simulation of MapReduce\n",
    "\n",
    "* Serial steps\n",
    "* No file distribution\n",
    "* Single node\n",
    "* Single mapper\n",
    "* Single reducer\n",
    "* Can we add a Combiner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Do you know how to substitute the awk/mapper with a python script?\n",
    "\n",
    "If yes, create one to count the most recurring words inside the Divine Comedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop streaming\n",
    "### Concepts and mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming is a utility \n",
    "\n",
    "* It comes bundled with the Hadoop distribution\n",
    "* It allows creating and running Map/Reduce jobs \n",
    "    - with any executable or script as the mapper and/or the reducer\n",
    "\n",
    "Protocol steps\n",
    "\n",
    "* Create a Map/Reduce job\n",
    "* Submit the job to an appropriate cluster\n",
    "* Monitor the progress of the job until it completes\n",
    "* Links to Hadoop HDFS job directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why?\n",
    "\n",
    "One of the most unappetizing aspects of Hadoop to users of traditional HPC is that it is written in Java. \n",
    "\n",
    "* Java is not originally designed to be a high-performance language\n",
    "* Learning Java is very difficult for domain scientists\n",
    "\n",
    "This is why Hadoop allows you to write map/reduce code in any language you want using the Hadoop Streaming interface\n",
    "\n",
    "* It means turning an existing Python or Perl script into a Hadoop job\n",
    "* Does not require learning any Java at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MapReduce streaming with binaries/executables\n",
    "\n",
    "* Executables are specified for mappers and reducers!\n",
    "    - each mapper task run as a separate process \n",
    "* Inputs converted into lines and feed to the `STDIN` of the process\n",
    "* The mapper collects `STDOUT` of the process \n",
    "    - each line is a key/value pair **separated by TAB**\n",
    "    - e.g. ”this is the key\\tvalue is the rest\\n”\n",
    "\n",
    "warning: If there is no tab character in the line, then entire line is considered as key and the value is null (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's do some live experiments..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A streaming command line example:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -input myInputDirs \\\n",
    "    -output myOutputDir \\\n",
    "    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \\\n",
    "    -reducer /bin/wc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A streaming command line example **for python**:\n",
    "``` bash\n",
    "$ hadoop jar $HADOOP_HOME/hadoop-streaming.jar \\\n",
    "    -files mapper.py,reducer.py\n",
    "    -input input_dir/ \\\n",
    "    -output output_dir/ \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before submitting the Hadoop Streaming job:\n",
    "\n",
    "* Make sure your scripts have no errors\n",
    "* Do mapper and reducer scripts actually work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is just a matter of running them through pipes on a **little bit** of sample data,\n",
    "\n",
    "like `cat` or `head` linux bash commands, with pipes, as seen before.\n",
    "\n",
    "```\n",
    "# Simulating hadoop streaming with bash pipes\n",
    "$ cat $file | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First approach: split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pieces = line.split('\\t')\n",
    "    print(pieces) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! head -n 150 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Count symbols (everything that is not a letter of the alphabet) inside a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Back to the example:\n",
    "\n",
    "Skip header lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "\n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "    print(mychr,mystart.__str__())\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! head -n 100 $myfile | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Produce an output that can be sorted and then used by reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "\n",
    "# Cycle current streaming data\n",
    "for line in sys.stdin:\n",
    "    # Clean input\n",
    "    line = line.strip()\n",
    "    # Skip SAM/BAM headers\n",
    "    if line[0] == \"@\":\n",
    "        continue\n",
    "    \n",
    "    # Use data\n",
    "    pieces = line.split(TAB)\n",
    "    mychr = pieces[2]\n",
    "    mystart = int(pieces[3])\n",
    "    myseq = pieces[9]\n",
    "\n",
    "    mystop = mystart + len(myseq)\n",
    "\n",
    "    # Each element with coverage\n",
    "    for i in range(mystart,mystop):\n",
    "        results = [mychr+SEP+i.__str__(), \"1\"]\n",
    "        print(TAB.join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! head -n 100 $myfile | python mapper.py | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shuffle step \n",
    "\n",
    "<br><big>\n",
    "A lot happens, transparent to the developer\n",
    "</big>\n",
    "\n",
    "* Mappers’s output is transformed and distributed to the reducers\n",
    "* All key/value pairs are sorted before sent to reducer function\n",
    "* Pairs sharing the same key are sent to the same reducer\n",
    "* If you encounter a key that is different from the last key you processed\n",
    "    - *you know that previous key will never appear again*\n",
    "* If your keys are all the same\n",
    "    - only use one reducer and gain no parallelization\n",
    "    - come up with a more unique key if this happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "TAB = \"\\t\"\n",
    "SEP = ':'\n",
    "import sys\n",
    "last_value = \"\"\n",
    "value_count = 1\n",
    "for line in sys.stdin:\n",
    "    value, count = line.strip().split(TAB)\n",
    "    # if this is the first iteration\n",
    "    if not last_value:\n",
    "        last_value = value\n",
    "    # if they're the same, log it\n",
    "    if value == last_value:\n",
    "        value_count += int(count)\n",
    "    else:\n",
    "        # state change\n",
    "        try: \n",
    "            print(TAB.join([last_value, str(value_count)]))\n",
    "        except:\n",
    "            pass\n",
    "        last_value = value\n",
    "        value_count = 1\n",
    "# LAST ONE after all records have been received\n",
    "print(TAB.join([last_value, str(value_count)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# needs ~ 5 seconds for running\n",
    "time head -n 10000 $myfile | python mapper.py | sort | python reducer.py | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "time cat $myfile | python mapper.py | sort | python reducer.py > out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "! grep \"\\s185\" out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Moving to real Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<big>\n",
    "A working python code tested on pipes **should work** with Hadoop Streaming\n",
    "</big>\n",
    "\n",
    "* To make this happen we need to handle copy of input and output files inside the Hadoop FS\n",
    "* Also the job tracker logs will be found inside HDFS\n",
    "* We are going to use bash scripting inside the notebook to make our workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HDFS commands to interact with Hadoop file system use the same syntax:\n",
    "\n",
    "```\n",
    "hdfs dfs -command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`command` are like bash commands for file\n",
    "\n",
    "e.g.\n",
    "\n",
    "```\n",
    "hadoop fs -mkdir hdfs:///dir\n",
    "hadoop fs -put file_on_host hdfs:///path/to/file\n",
    "hadoop fs -ls\n",
    "```\n",
    "\n",
    "<small>\n",
    "Note: we have seen this in action with the Java example\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<big>\n",
    "Hadoop Streaming needs “binaries” to execute\n",
    "</big>\n",
    "\n",
    "You need to specify interpreter at the beginning of your scripts:\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "```\n",
    "\n",
    "Make also the script executables:\n",
    "```\n",
    "chmod +x hs*.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! chmod +x mapper.py reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%env HADOOP_STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Launch streaming\n",
    "hadoop jar $HADOOP_STREAMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Preprocess with HDFS\n",
    "hdfs dfs -rm -r -f myinput\n",
    "hdfs dfs -mkdir myinput\n",
    "# Save one file inside\n",
    "file=\"/tmp/ngs.sam\"\n",
    "hdfs dfs -put $file myinput/file01\n",
    "# Remove output or Hadoop will give error if existing\n",
    "hdfs dfs -rm -r -f myoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Final launch via bash command for using Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "# A real Hadoop Streaming run\n",
    "time hadoop jar $HADOOP_STREAMING \\\n",
    "    -D mapreduce.job.mapper=12 -D mapreduce.job.reducers=4  \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -input myinput -output myoutput \\\n",
    "    -mapper mapper.py -reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hadoop streaming is **difficult to debug**.\n",
    "Just like real Java Hadoop.\n",
    "\n",
    "If you did a typical setup mistake, you may end receiving unrelated errors stacktrace from the Java virtual machine.\n",
    "\n",
    "So before googling those stacktrace, make sure that:\n",
    "\n",
    "* Python files (mapper and reducer) exists\n",
    "* They are provided inside the main bash command also as **files** list\n",
    "* They are executables and contain as first line the hashbang\n",
    "* Your input directory exists on HDFS\n",
    "* Files inside your input directory are not corrupted\n",
    "    - e.g. bad decompression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# OUTPUT: Check directory\n",
    "! hdfs dfs -ls myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# OUTPUT:  Copy file and go see it\n",
    "! rm -rf hs.*.txt && hdfs dfs -get myoutput/part-00000 hs.out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! head hs.out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final thoughts on Hadoop Streaming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* Provides options to write MapReduce jobs in other languages\n",
    "* Even executables can be used to work as a MapReduce job\n",
    "* One of the best examples of flexibility available to MapReduce\n",
    "* Fast\n",
    "* Simpler than Java\n",
    "* Also close to the original standard Java API Hadoop power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Where it really works\n",
    "\n",
    "* When the developer do not have knowhow of Java \n",
    "* Write Mapper/Reducer in any scripting language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Disadvantages\n",
    "\n",
    "* Force scripts in a Java VM\n",
    "    - Although almost free overhead\n",
    "* The program/executable have to take input from STDIN \n",
    "    - and produce output at STDOUT\n",
    "* Restrictions on the input/output formats\n",
    "    - Does not take care of input and output file and directory preparation\n",
    "    - User have to implement hdfs commands “hand-made”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Where it falls short\n",
    "\n",
    "* No pythonic way to work the MapReduce code\n",
    "\n",
    "(Because it was not written specifically for python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap \n",
    "\n",
    "* Hadoop streaming handles Hadoop in almost a classic manner\n",
    "* Wrap any executable (and script)\n",
    "    - Also python scripts\n",
    "* Runnable on a cluster using a non-interactive, all-encapsulated job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of this Chapter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
