{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster analysis example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MLlib libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.clustering._\n",
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.SparkContext._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by Ronald Fisher in his 1936 paper \"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\".\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rawData = sc.textFile(\"data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "rawData.take(5).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following  code splits the CSV lines into columns and removes the final column. The remaining values are converted to an array of numeric values (Double objects), and emitted with the final label column in a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iris-setosa,50)\n",
      "(Iris-virginica,50)\n",
      "(Iris-versicolor,50)\n"
     ]
    }
   ],
   "source": [
    "rawData.map(_.split(',').last).countByValue().toSeq.sortBy(_._2).reverse.foreach(println)\n",
    "val labelsAndData = rawData.map { line =>\n",
    "      val buffer = line.split(',').toBuffer\n",
    "      val label = buffer.remove(buffer.length - 1)\n",
    "      val vector = Vectors.dense(buffer.map(_.toDouble).toArray)\n",
    "      (label, vector)\n",
    "                                }\n",
    "val data = labelsAndData.values.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. The MLlib implementation includes a parallelized variant of the k-means++ method called kmeans||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val numClusters = 2\n",
    "val numIterations = 1\n",
    "val model = KMeans.train(data,numClusters,numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.273737373737372,2.8757575757575764,4.925252525252526,1.6818181818181817]\n",
      "[5.007843137254901,3.409803921568628,1.4921568627450983,0.2627450980392156]\n"
     ]
    }
   ],
   "source": [
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cluster model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the model to assign each observation to a cluster, counts occurrences of cluster and label pairs, and prints them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   Iris-versicolor      47\n",
      "0    Iris-virginica      50\n",
      "1       Iris-setosa      50\n",
      "1   Iris-versicolor       3\n"
     ]
    }
   ],
   "source": [
    "val clusterLabelCount = labelsAndData.map { case (label, datum) =>\n",
    "    val cluster = model.predict(datum)\n",
    "    (cluster, label)}.countByValue()\n",
    "    clusterLabelCount.toSeq.sorted.foreach { case ((cluster, label), count) =>\n",
    "    println(f\"$cluster%1s$label%18s$count%8s\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write sample and total result in a directory hadoop style and coalesce all in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sample = data.map(datum => model.predict(datum) + \",\" +datum.toArray.mkString(\",\")).sample(false,0.05)\n",
    "val total = data.map(datum => model.predict(datum) + \",\" +datum.toArray.mkString(\",\"))\n",
    "total.saveAsTextFile(\"results/total2\")\n",
    "total.coalesce(1).saveAsTextFile(\"results/sample_total2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Choice of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clustering could be considered good if each data point were near to its closest centroid. So, we define a Euclidean distance function, and a function that returns the distance from a data point to its nearest cluster’s centroid. From this, it’s possible to define a function that measures the average distance to centroid, for a model built with a given k.\n",
    "This is an internal quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def distance(a: Vector, b: Vector) =math.sqrt(a.toArray.zip(b.toArray).map(p => p._1 - p._2).map(d => d * d).sum)\n",
    "def distToCentroid(datum: Vector, model: KMeansModel) = {\n",
    "      val cluster = model.predict(datum)\n",
    "      val centroid = model.clusterCenters(cluster)\n",
    "      distance(centroid, datum)\n",
    "}   \n",
    "def clusteringScore(data: RDD[Vector], k: Int): Double = {\n",
    "      val kmeans = new KMeans()\n",
    "      kmeans.setK(k)\n",
    "      kmeans.setRuns(30)\n",
    "      kmeans.setEpsilon(1.0e-6)\n",
    "      val model = kmeans.run(data)\n",
    "      data.map(datum => distToCentroid(datum, model)).mean()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate score values for different k from 1 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1.9440683605553895)\n",
      "(2,0.855577769526653)\n",
      "(3,0.6480304904934434)\n",
      "(4,0.5573847727333313)\n",
      "(5,0.5096521951118287)\n",
      "(6,0.4670733963396439)\n",
      "(7,0.4381125733411079)\n",
      "(8,0.4108698632459714)\n",
      "(9,0.3961705272186211)\n",
      "(10,0.388696499714614)\n"
     ]
    }
   ],
   "source": [
    "(1 to 10 by 1).map(k => (k, clusteringScore(data, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Euclidean distance is used, the clusters will be influenced strongly by the magnitudes of the variables, especially by outliers. Normalizing removes this bias. We can normalize each feature by converting it to a standard score. This means subtracting the mean of the feature’s values from each value, and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildNormalizationFunction(data: RDD[Vector]): (Vector => Vector) = {\n",
    "    val dataAsArray = data.map(_.toArray)\n",
    "    val numCols = dataAsArray.first().length\n",
    "    val n = dataAsArray.count()\n",
    "    val sums = dataAsArray.reduce(\n",
    "      (a, b) => a.zip(b).map(t => t._1 + t._2))\n",
    "    val sumSquares = dataAsArray.fold(\n",
    "        new Array[Double](numCols)\n",
    "      )(\n",
    "        (a, b) => a.zip(b).map(t => t._1 + t._2 * t._2)\n",
    "      )\n",
    "    val stdevs = sumSquares.zip(sums).map {\n",
    "      case (sumSq, sum) => math.sqrt(n * sumSq - sum * sum) / n\n",
    "    }\n",
    "    val means = sums.map(_ / n)\n",
    "\n",
    "    (datum: Vector) => {\n",
    "      val normalizedArray = (datum.toArray, means, stdevs).zipped.map(\n",
    "        (value, mean, stdev) =>\n",
    "          if (stdev <= 0)  (value - mean) else  (value - mean) / stdev\n",
    "      )\n",
    "      Vectors.dense(normalizedArray)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val normalizedData = data.map(buildNormalizationFunction(data)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of K with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,0.034363366543011024)\n",
      "(2,0.01477149695109605)\n",
      "(3,0.008824012747163202)\n",
      "(4,0.007194540980690931)\n",
      "(5,0.006304525208280512)\n",
      "(6,0.005749320505622912)\n",
      "(7,0.005135446289737331)\n",
      "(8,0.004699770499438223)\n",
      "(9,0.00442993146456794)\n",
      "(10,0.004269386854148335)\n"
     ]
    }
   ],
   "source": [
    "(1 to 10 by 1).map(k =>\n",
    "      (k, clusteringScore(normalizedData, k))).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Entropy measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are different  metrics for homogeneity. Entropy is used here for illustration. A good clustering would have clusters whose collections of labels are homogeneous and so have low entropy. A weighted average of entropy can therefore be used as a cluster score.\n",
    "This is an external quality measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(counts: Iterable[Int]) = {\n",
    "    val values = counts.filter(_ > 0)\n",
    "    val n: Double = values.sum\n",
    "    values.map { v =>\n",
    "    val p = v / n \n",
    "    -p * math.log(p)\n",
    "    }.sum\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildCategoricalAndLabelFunction(rawData: RDD[String]): (String => (String,Vector)) = {\n",
    "val splitData = rawData.map(_.split(','))\n",
    "   (line: String) => {\n",
    "      val buffer = line.split(',').toBuffer \n",
    "      val label = buffer.remove(buffer.length - 1)\n",
    "      val vector = buffer.map(_.toDouble)\n",
    "      (label, Vectors.dense(vector.toArray))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for choosing K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,1.0986122886681096)\n",
      "(2,0.4620981203732969)\n",
      "(3,0.16554309474734444)\n",
      "(4,0.16368731464664396)\n",
      "(5,0.12442865073449615)\n",
      "(6,0.13095513392785052)\n",
      "(7,0.11730722141295463)\n",
      "(8,0.11730722141295463)\n",
      "(9,0.11346845541210213)\n",
      "(10,0.10606250990139728)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[1000] at mapValues at <console>:49"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusteringScore3(normalizedLabelsAndData: RDD[(String,Vector)], k: Int) = {\n",
    "    val kmeans = new KMeans()\n",
    "    kmeans.setK(k)\n",
    "    kmeans.setRuns(10)\n",
    "    kmeans.setEpsilon(1.0e-6)\n",
    "    val model = kmeans.run(normalizedLabelsAndData.values)\n",
    "    val labelsAndClusters = normalizedLabelsAndData.mapValues(model.predict)\n",
    "    val clustersAndLabels = labelsAndClusters.map(_.swap)\n",
    "    val labelsInCluster = clustersAndLabels.groupByKey().values\n",
    "    val labelCounts = labelsInCluster.map(_.groupBy(l => l).map(_._2.size))\n",
    "    val n = normalizedLabelsAndData.count()\n",
    "    labelCounts.map(m => m.sum * entropy(m)).sum / n\n",
    "}\n",
    "\n",
    "val parseFunction = buildCategoricalAndLabelFunction(rawData)\n",
    "val labelsAndData = rawData.map(parseFunction)\n",
    "val normalizedLabelsAndData =\n",
    "      labelsAndData.mapValues(buildNormalizationFunction(labelsAndData.values)).cache()\n",
    "(1 to 10 by 1).map(k =>\n",
    "      (k, clusteringScore3(normalizedLabelsAndData, k))).foreach(println)\n",
    "normalizedLabelsAndData.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "checked"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
