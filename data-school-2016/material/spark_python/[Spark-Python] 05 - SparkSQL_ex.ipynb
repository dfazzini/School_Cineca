{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Py)SparkSQL Exercizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Try to solve the following exercize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download file at: \n",
    "FILE_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4350eb7ff028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msqlCtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#sqlCtx.sql(\"SET spark.sql.parquet.binaryAsString=true\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#sqlCtx.sql(\"SET spark.sql.inMemoryColumnarStorage.compressed=true\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A SQLConext wraps the SparkContext,\n",
    "and adds functions for working with structured data.\n",
    "'''\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlCtx = SQLContext(sc)\n",
    "#sqlCtx.sql(\"SET spark.sql.parquet.binaryAsString=true\")\n",
    "#sqlCtx.sql(\"SET spark.sql.inMemoryColumnarStorage.compressed=true\")\n",
    "sqlCtx.sql(\"SET spark.sql.parquet.useDataSourceApi=false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SQLContext and data types\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load a set of data in that is stored in the Parquet format.\n",
    "Parquet is a self-describing columnar format. Since it is self-describing,\n",
    "Spark SQL will automatically be able to infer all of the column names and their datatypes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiData = sqlCtx.parquetFile(\"./data/wiki_parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of loading in a parquet file is a SchemaRDD.\n",
    "A SchemaRDD has all of the functions of a normal RDD.\n",
    "For example, lets figure out how many records are in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to standard RDD operatrions, SchemaRDDs also have\n",
    "extra information about the names and types of the columns in the dataset.\n",
    "This extra schema information makes it possible to run SQL queries against\n",
    "the data after you have registered it as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to describe the schema\n",
    "wikiData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# number of record (again)\n",
    "result_df = sqlCtx.sql(\"SELECT COUNT(*) AS pageCount FROM wikiData\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = result_df.collect()\n",
    "result[0].pageCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL can be a powerfull tool from performing complex aggregations.\n",
    "For example, the following query returns the top 10 usersnames by the number of pages\n",
    "they created.\n",
    "\n",
    "```\n",
    "sqlCtx.sql(\"SELECT username, COUNT(*) AS cnt FROM wikiData WHERE username <> '' GROUP BY username ORDER BY cnt DESC LIMIT 10\").collect()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx.sql(\"SELECT username, COUNT(*) AS cnt\\\n",
    "            FROM wikiData\\\n",
    "            WHERE username <> ''\\\n",
    "            GROUP BY username\\\n",
    "            ORDER BY cnt DESC LIMIT 10\")\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `LIKE` operator (inside the SQL query),\n",
    "select the id and title of the wiki articles\n",
    "that contain both **italy** and **bologna** in its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wikiData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#sqlCtx.sql(\"SELECT id AS ID, title AS Title FROM wikiData WHERE test LIKE 'bologna' LIMIT 10\").toPandas()\n",
    "sqlCtx.sql(\"SELECT id, title\\\n",
    "            FROM wikiData\\\n",
    "            WHERE text LIKE '%bologna%'\\\n",
    "            AND text LIKE '%italy%'\\\n",
    "            LIMIT 10\")\\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
