{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful DataFrame functions\n",
    "\n",
    "1. Random Data Generation\n",
    "2. Summary and Descriptive Statistics\n",
    "3. Sample covariance and correlation\n",
    "4. Contingency Table\n",
    "5. Frequent Items\n",
    "\n",
    "\n",
    "## 1 - Random Data Generation\n",
    "Random data generation is useful for testing of existing algorithms and implementing randomized algorithms, such as random projection.\n",
    "\n",
    "Spark provides methods under sql.functions for generating columns that contains i.i.d. values drawn from a distribution, e.g., uniform (rand),  and standard normal (randn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with one int column and 10 rows.\n",
    "df = sqlContext.range(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787|  0.5888539012978773|\n",
      "|  1| 0.1982919638208397| 0.06157382353970104|\n",
      "|  2|0.12030715258495939|  1.0854146699817222|\n",
      "|  3|0.44292918521277047| -0.4798519469521663|\n",
      "|  4| 0.8898784253886249| -0.8820294772950535|\n",
      "|  5| 0.2731073068483362|-0.15116027592854422|\n",
      "|  6|   0.87079354700073|-0.27674189870783683|\n",
      "|  7|0.27149331793166864|-0.18575112254167045|\n",
      "|  8| 0.6037143578435027|   0.734722467897308|\n",
      "|  9| 0.1435668838975337|-0.30123700668427145|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate two other columns using uniform distribution and normal distribution.\n",
    "df.select(\"id\", rand(seed=10).alias(\"uniform\"), randn(seed=27).alias(\"normal\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Summary and Descriptive Statistics\n",
    "- The first operation to perform after importing data is to get some sense of what it looks like.\n",
    "- For numerical columns, knowing the descriptive summary statistics can help a lot in understanding the distribution of your data.\n",
    "- The function describe returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+--------------------+\n",
      "|summary|                id|            uniform|              normal|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "|  count|                10|                 10|                  10|\n",
      "|   mean|               4.5|0.42277947877387234|0.019379313460706583|\n",
      "| stddev|3.0276503540974917|0.28230337640258524|  0.6053138209606246|\n",
      "|    min|                 0|0.12030715258495939| -0.8820294772950535|\n",
      "|    max|                 9| 0.8898784253886249|  1.0854146699817222|\n",
      "+-------+------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A slightly different way to generate the two random columns\n",
    "df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27))\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a DataFrame with a large number of columns, you can also run describe on a subset of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+\n",
      "|summary|            uniform|              normal|\n",
      "+-------+-------------------+--------------------+\n",
      "|  count|                 10|                  10|\n",
      "|   mean|0.42277947877387234|0.019379313460706583|\n",
      "| stddev|0.28230337640258524|  0.6053138209606246|\n",
      "|    min|0.12030715258495939| -0.8820294772950535|\n",
      "|    max| 0.8898784253886249|  1.0854146699817222|\n",
      "+-------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('uniform', 'normal').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, while describe works well for quick exploratory data analysis, you can also control the list of descriptive statistics and the columns they apply to using the normal select on a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+\n",
      "|       avg(uniform)|       min(uniform)|      max(uniform)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|0.42277947877387234|0.12030715258495939|0.8898784253886249|\n",
      "+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "df.select([mean('uniform'), min('uniform'), max('uniform')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Sample covariance and correlation\n",
    "- Covariance is a measure of how two variables change with respect to each other.\n",
    "- A positive number would mean that there is a tendency that as one variable increases, the other increases as well.\n",
    "- A negative number would mean that as one variable increases, the other variable has a tendency to decrease.\n",
    "- The sample covariance of two columns of a DataFrame can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.range(0, 10).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.011320674637528958"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov('rand1', 'rand2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.166666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov('id', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above, the covariance of the two randomly generated columns is close to zero, while the covariance of the id column with itself is very high.\n",
    "\n",
    "The covariance value of 9.17 might be hard to interpret. Correlation is a normalized measure of covariance that is easier to understand, as it provides quantitative measurements of the statistical dependence between two random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15104231216965627"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('rand1', 'rand2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr('id', 'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, id correlates perfectly with itself, while the two randomly generated columns have low correlation value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Contingency Table\n",
    "- Cross Tabulation provides a table of the frequency distribution for a set of variables.\n",
    "- Cross-tabulation is a powerful tool in statistics that is used to observe the statistical significance (or independence) of variables.\n",
    "- In Spark, users will be able to cross-tabulate two columns of a DataFrame in order to obtain the counts of the different pairs that are observed in those columns.\n",
    "\n",
    "Here is an example on how to use crosstab to obtain the contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with two columns (name, item)\n",
    "names = [\"Alice\", \"Bob\", \"Mike\"]\n",
    "items = [\"milk\", \"bread\", \"butter\", \"apples\", \"oranges\"]\n",
    "df = sqlContext.createDataFrame([(names[i % 3], items[i % 5]) for i in range(100)], [\"name\", \"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "| name|   item|\n",
      "+-----+-------+\n",
      "|Alice|   milk|\n",
      "|  Bob|  bread|\n",
      "| Mike| butter|\n",
      "|Alice| apples|\n",
      "|  Bob|oranges|\n",
      "| Mike|   milk|\n",
      "|Alice|  bread|\n",
      "|  Bob| butter|\n",
      "| Mike| apples|\n",
      "|Alice|oranges|\n",
      "+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 10 rows.\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------+----+-----+\n",
      "|name_item|apples|oranges|butter|milk|bread|\n",
      "+---------+------+-------+------+----+-----+\n",
      "|      Bob|     6|      7|     7|   6|    7|\n",
      "|     Mike|     7|      6|     7|   7|    6|\n",
      "|    Alice|     7|      7|     6|   7|    7|\n",
      "+---------+------+-------+------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab(\"name\", \"item\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to keep in mind is that the cardinality of columns we run crosstab on cannot be too big. That is to say, the number of distinct “name” and “item” cannot be too large. Just imagine if “item” contains 1 billion distinct entries: how would you fit that table on your screen?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Frequent Items\n",
    "Figuring out which items are frequent in each column can be very useful to understand a dataset.\n",
    "\n",
    "In Spark, users will be able to find the frequent items for a set of columns using DataFrames. Spark has an one-pass algorithm proposed by Karp et al. This is a fast, approximate algorithm that always return all the frequent items that appear in a user-specified minimum proportion of rows.\n",
    "\n",
    "Note that the result might contain false positives, i.e. items that are not frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above DataFrame, the following code finds the frequent items that show up 40% of the time for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(a_freqItems=[11, 1], b_freqItems=[2, 22], c_freqItems=[1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)\n",
    "freq.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, “11” and “1” are the frequent values for column “a”. You can also find frequent items for column combinations, by creating a composite column using the struct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ab_freqItems=[Row(a=11, b=22), Row(a=1, b=2)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "freq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n",
    "freq.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above example, the combination of “a=11 and b=22”, and “a=1 and b=2” appear frequently in this dataset. Note that “a=11 and b=22” is a false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
