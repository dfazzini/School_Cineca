{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Py)Spark Exercises\n",
    "\n",
    "_Giovanni Simonini - Giuseppe Fiameni_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark vs Pyspark?\n",
    "\n",
    "Spark is written in Scala. The 'native' API is in Scala.\n",
    "\n",
    "_Pyspark_ is a lightweight wrapper around the native API. (You can see its implementation [here](https://github.com/apache/spark/tree/master/python/pyspark).)\n",
    "\n",
    "---\n",
    "\n",
    "![](http://i.imgur.com/YlI8AqEl.png)\n",
    "\n",
    "[source](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to the (py)Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Warm up by creating an RDD (Resilient Distributed Dataset) named `pagecounts` from the input files.\n",
    "\n",
    "Let's create the SparkContext `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "'''\n",
    "if a spark context is already available this snippet fails\n",
    "'''\n",
    "#PATH = \"/notebooks/cineca/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pagecounts = sc.textFile(\"./data/pagecounts\")\n",
    "\n",
    "# ! head -n 1000 ./data/pagecounts | cut -d' ' -f 2 | uniq | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's take a peek at the data. You can use the take operation of an RDD to get the first N records. Here, N = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pagecounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unfortunately this is not very readable because `take()` returns an array and Python simply prints the array with each element separated by a comma.\n",
    "We can make it prettier by traversing the array to print each record on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for x in pagecounts.take(10):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see how many records in total are in this data set (this command will take a while, so read ahead while it is running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cache in memory\n",
    "Recall from above when we described the format of the data set, that the second field is the \"project code\" and contains information about the language of the pages.\n",
    "\n",
    "For example, the project code \"en\" indicates an English page.\n",
    "   Let's derive an RDD containing only English pages from `pagecounts`.\n",
    "   This can be done by applying a filter function to `pagecounts`.\n",
    "   For each record, we can split it by the field delimiter (i.e. a space) and get the second field-â€“ and then compare it with the string \"en\".\n",
    "\n",
    "   To avoid reading from disks each time we perform any operations on the RDD, we also __cache the RDD into memory__.\n",
    "    This is where Spark really starts to to shine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enPages = pagecounts.filter(lambda x: x.split(\" \")[1] == \"en\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When you type this command into the Spark shell, Spark defines the RDD, but because of lazy evaluation, no computation is done yet.\n",
    "   Next time any action is invoked on `enPages`, Spark will cache the data set in memory across the workers in your cluster.\n",
    "\n",
    "**How many records are there for English pages?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enPages.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first time this command is run, similar to the last count we did, it will take 2 - 3 minutes while Spark scans through the entire data set on disk.\n",
    "   __But since `enPages` was marked as \"cached\" in the previous step, if you run count on the same RDD again, it should return an order of magnitude faster__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enPages.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD Persistency levels\n",
    "\n",
    "*Storage Level* | *Meaning*\n",
    "--- | ---\n",
    "MEMORY_ONLY | Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.\n",
    "MEMORY_AND_DISK | Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.\n",
    "MEMORY_ONLY_SER | Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\n",
    "MEMORY_AND_DISK_SER | Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.\n",
    "DISK_ONLY|\tStore the RDD partitions only on disk.\n",
    "MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc. | Same as the levels above, but replicate each partition on two cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> `RDD.persist(MEMORY_ONLY)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Let's try something fancier.**\n",
    "\n",
    "Generate a histogram of total page views on Wikipedia English pages for the date range represented in our dataset (May 5 to May 7, 2009). The high level idea of what we'll be doing is as follows. First, we generate a key value pair for each line; the key is the date (the first eight characters of the first field), and the value is the number of pageviews for that date (the fourth field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enTuples = enPages.map(lambda x: x.split(\" \"))\n",
    "enKeyValuePairs = enTuples.map(lambda x: (x[0][:8], int(x[3])))\n",
    "enKeyValuePairs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we shuffle the data and group all values of the same key together.\n",
    "Finally we sum up the values for each key.\n",
    "There is a convenient method called `reduceByKey` in Spark for exactly this pattern.\n",
    "\n",
    "Note that the second argument to `reduceByKey` determines the number of reducers to use.\n",
    "By default, Spark assumes that the reduce function is commutative and associative and applies combiners on the mapper side.\n",
    "Since we know there is a very limited number of keys in this case (because there are only 3 unique dates in our data set), let's use only one reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enKeyValuePairs.reduceByKey(lambda x, y: x + y, 1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `collect` method at the end converts the result from an RDD to an `array`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can combine the previous three commands into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enPages.map(lambda x: x.split(\" \"))\\\n",
    "        .map(lambda x: (x[0][:8], int(x[3])))\\\n",
    "        .reduceByKey(lambda x, y: x + y, 1)\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we want to find pages that were viewed more than 200,000 times during the three days covered by our dataset. Conceptually, this task is similar to the previous query. But, given the large number of pages the new task is very expensive. We are doing an expensive group-by with a lot of network shuffling of data.\n",
    "\n",
    "To recap:\n",
    "   1. _first we split each line of data into its fields_\n",
    "   2. _we extract the fields for page name and number of page views_\n",
    "   3. _we reduce by key again, this time with 40 reducers_\n",
    "   4. _then we filter out pages with less than 200,000 total views over our time window represented by our dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "enPages.map(lambda x: x.split(\" \"))\\\n",
    "        .map(lambda x: (x[2], int(x[3])))\\\n",
    "        .reduceByKey(lambda x, y: x + y, 40)\\\n",
    "        .filter(lambda x: x[1] > 200000)\\\n",
    "        .map(lambda x: (x[1], x[0]))\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is no hard and fast way to calculate the optimal number of reducers for a given problem; you will build up intuition over time by experimenting with different values.\n",
    "\n",
    "\n",
    "You can explore the full RDD API by browsing the [Java/Scala](http://www.cs.berkeley.edu/~pwendell/strataconf/api/core/index.html#spark.RDD) or [Python](http://www.cs.berkeley.edu/~pwendell/strataconf/api/pyspark/index.html) API docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PySpark API\n",
    "\n",
    "Let's look at some PySpark API.\n",
    "\n",
    "> By clicking on the images you can directly access to the documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.map\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page3.svg\" width=400 height=400 />\n",
    "</a>\n",
    "\n",
    "\n",
    "\n",
    "__`map(func)`__\tReturn a new distributed dataset formed by passing each element of the source through a function func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# map\n",
    "x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object\n",
    "print(x.collect())  # collect copies RDD elements to a list on the driver\n",
    "\n",
    "y = x.map(lambda x: (x,x**2))\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMap\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page4.svg\" width=400 height=400/>\n",
    "</a>\n",
    "\n",
    "__`flatMap(func)`__\tSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([1,2,3])\n",
    "print(x.collect())\n",
    "\n",
    "def f(x):\n",
    "    return [100*x, x**2]\n",
    "print(x.map(f).collect()) # Map\n",
    "\n",
    "y = x.flatMap(\n",
    "    lambda x: (x, 100*x, x**2)) # this lambda yields 2 elements for each element of x\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page8.svg\" width=400 height=400 />\n",
    "</a>\n",
    "\n",
    "__`filter(func)`__\tReturn a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "print(x.collect())\n",
    "\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page23.svg\" width=400 height=400/>\n",
    "</a>\n",
    "\n",
    "__`reduce(func)`__\tAggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# reduce\n",
    "x = sc.parallelize([1,2,3])\n",
    "print(x.collect())\n",
    "\n",
    "y = x.reduce(lambda obj, accumulated: obj + accumulated)  # computes a cumulative sum\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page44.svg\" width=400 height=400/>\n",
    "</a>\n",
    "\n",
    "__`reduceByKey(func, [numTasks])`__\tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "print(x.collect())\n",
    "\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortBy\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page15.svg\" width=400 height=400 />\n",
    "</a>\n",
    "\n",
    "__`sortByKey([ascending], [numTasks])`__\tWhen called on a dataset of `(K, V)` pairs where K implements Ordered, returns a dataset of `(K, V)` pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# sortBy\n",
    "x = sc.parallelize(['Cat','Apple','Bat'])\n",
    "print(x.collect())\n",
    "\n",
    "def keyGen(val):\n",
    "    return val[0]\n",
    "\n",
    "y = x.sortBy(keyGen)\n",
    "\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page39.svg\" width=400 height=400 />\n",
    "</a>\n",
    "\n",
    "__`take(n)`__ Return an array with the first n elements of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,3,1,2,3])\n",
    "print(x.collect())\n",
    "\n",
    "y = x.take(num = 3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
    "<img align=center src=\"./pyspark_API/images/pyspark-page9.svg\" width=400 height=400 />\n",
    "</a>\n",
    "\n",
    "__`distinct([numTasks]))`__\tReturn a new dataset that contains the distinct elements of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# distinct\n",
    "x = sc.parallelize(['A','A','B','A','B'])\n",
    "print(x.collect())\n",
    "\n",
    "y = x.distinct()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End of this chapter"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
